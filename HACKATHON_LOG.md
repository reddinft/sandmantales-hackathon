# Sandman Tales â€” Hackathon Build Log
_Team ClawCutters | Mistral AI Hackathon | UNSW Founders, Sydney | Feb 28â€“Mar 1, 2026_
_Solo entry by Nissan Dookeran (@redditech) | Orchestrated by 4 AI agents via Mistral Agents API_

---

## Timeline

### Day 1 â€” Saturday Feb 28

#### 14:00â€“15:00 AEST â€” Planning & Agent Setup
- Created 4 Mistral Agents via API: Doc (orchestrator), Pathfinder (story gen), Firefly (builder), Lifeline (voice/audio)
- Each agent has a distinct personality and accent: Doc (Trinidad ğŸ‡¹ğŸ‡¹), Pathfinder (Samurai ğŸ‡¯ğŸ‡µ), Firefly (Australian ğŸ‡¦ğŸ‡º), Lifeline (Kiwi ğŸ‡³ğŸ‡¿)
- Wrote SOUL.md personality files for all 4 agents
- Agent IDs stored in `mistral_agents.json`
- **Decision:** Fresh repo `reddinft/sandmantales-hackathon` â€” no legacy code from old project

#### 15:00â€“15:30 â€” The Orchestration Problem (The Gotcha)
- Spawned Doc as lead orchestrator. He was supposed to delegate to Pathfinder, Firefly, Lifeline via Mistral Agents API
- **Problem:** Doc tried to do EVERYTHING himself. Wrote code, generated content, made architecture decisions â€” never called his team agents once
- First session: timed out at 10 minutes. Second: failed at 7 minutes
- **Root cause:** Delegation wasn't easy enough. An orchestrator without frictionless delegation tools is just a fancy single agent
- **Fix:** Built `team.py` â€” a CLI tool (generated by Vibe CLI/devstral-2) that lets Doc call any agent with `python3 team.py pathfinder "message"`
- Third spawn with team.py: **Doc successfully delegated** to all 3 agents
- **Key insight for talk:** "We built team.py to make delegation easier than doing it yourself. That's the core lesson of multi-agent orchestration."

#### 15:30â€“16:00 â€” Phase 1: Agent Outputs
- **Pathfinder** â†’ Generated "Luna the Brave Little Fox" (6 scenes, structured JSON with title, text, mood, illustration_prompt per scene)
- **Firefly** â†’ Delivered 26-file flat architecture plan for FastAPI + React
- **Lifeline** â†’ Researched ElevenLabs multilingual voice config: recommended voices, models, stability/similarity settings per language (EN/JA/FR/HI)
- **Doc** â†’ Created 11 GitHub issues, started backend scaffold
- All communication via Mistral Conversations API â€” provably Mistral-generated

#### 16:00â€“16:30 â€” Phase 1 Demo Video
- Generated ElevenLabs voice segments for all 4 agents (each in their accent)
- Combined into 2:28 demo narration with FLUX-generated character avatars
- Avatars: Doc=Scarlet Ibis, Pathfinder=Kitsune Fox, Firefly=Platypus, Lifeline=Kiwi Bird
- All avatars generated locally via FLUX schnell 512x512 on Mac Mini M4

#### 16:30â€“16:50 â€” Doc Phase 2: The Build
- Respawned Doc with explicit step-by-step build instructions
- Doc used **Vibe CLI (devstral-2)** for ALL code generation â€” not Claude, not GPT
- Backend: FastAPI + aiosqlite, endpoints for story gen + narration + CRUD
- Frontend: React + Vite + TypeScript + Tailwind CSS (dark theme, indigo accents)
- Doc called team.py to get Pathfinder stories and Lifeline voice recommendations
- **4 commits pushed** by Doc in one session

#### 16:50â€“17:15 â€” E2E Testing & Bug Fixes
Critical bugs found and fixed during testing:

1. **Mistral FunctionCallEntry bug:** Pathfinder has a function tool defined, so instead of returning text content, the API returns a `FunctionCallEntry` with story data in `.arguments`. Both `team.py` and `main.py` assumed `.content` existed â†’ crashed with `'FunctionCallEntry' object has no attribute 'content'`
   - **Fix:** Check `type(output).__name__` and handle both `FunctionCallEntry` (parse `.arguments`) and `MessageOutputEntry` (parse `.content`)
   - **Lesson:** Mistral Agents with function tools return structured data differently. Document your agent's tools or you'll hit this.

2. **aiosqlite threading bug:** `async with await get_db() as db:` was reusing the same `aiosqlite.connect()` object, which starts an internal thread. Second call â†’ `RuntimeError: threads can only be started once`
   - Spent 20 minutes thinking this was a Mistral SDK threading issue
   - **Fix:** Use `aiosqlite.connect(DB_PATH)` directly in each endpoint, not a shared helper
   - **Lesson:** aiosqlite connections are NOT reusable across async contexts. Create fresh connections.

3. **Mistral SDK + asyncio incompatibility:** Even after fixing aiosqlite, calling the Mistral Python SDK inside a FastAPI async endpoint caused threading conflicts
   - **Fix:** Shell out to `team.py` via `asyncio.create_subprocess_exec()` â€” the SDK works fine in its own process
   - **Lesson:** Some SDKs with internal threading don't play nice with asyncio event loops. Subprocess isolation is a legitimate pattern.

4. **Pydantic response model mismatch:** `Story.content` was defined as `str` but we return a dict with scenes. FastAPI's `response_model=Story` validation rejected it
   - **Fix:** Remove `response_model` constraint from the story endpoint, return free-form JSON
   - **Lesson:** Don't over-constrain response models when your data shape is richer than your schema

5. **Missing StreamingResponse import:** Narration endpoint returned audio bytes but forgot to import `StreamingResponse` from Starlette
   - One-liner fix, but it's the kind of thing that costs 5 minutes of "why is my API returning JSON instead of audio?"

6. **ElevenLabs voice_not_found:** Lifeline recommended voice IDs that exist in ElevenLabs' shared library but weren't added to our account. JA/FR/HI voices returned 404
   - **Fix:** Use Adam (premade, available in every account) with `eleven_multilingual_v2` model â€” it speaks all languages perfectly
   - **Lesson:** ElevenLabs shared library voices need to be explicitly added to your account. Premade voices "just work"

All 5 E2E tests passing after fixes:
- âœ… POST /api/story â†’ Pathfinder â†’ Mistral Agents API
- âœ… GET /api/stories (list)
- âœ… GET /api/stories/:id (detail)
- âœ… POST /api/narrate â†’ ElevenLabs (95KB MP3)
- âœ… Frontend dev server on :5173

#### 17:15â€“17:40 â€” Demo Stories & Illustrations
- Generated 3 demo stories via the live API:
  - **EN:** "Mia and the Glowing Garden of Whispers" (6 scenes)
  - **JA:** "æœˆæ˜ã‹ã‚Šã®ç´„æŸï¼šã‚†ãã¨å°ã•ãªãã¤ã­ã®å†’é™º" (6 scenes)
  - **FR:** "Sophie et la Baleine de Nuages" (6 scenes)
- All stories generated by Pathfinder via Mistral Conversations API â€” fully traceable
- Generated FLUX illustrations for scene 1 of each story:
  - EN: White kitten in moonlit garden of glowing flowers (490KB)
  - JA: White fox cub in snowy moonlit forest (487KB)
  - FR: Girl riding a cloud whale through starry sky (456KB)
- Added static file serving to FastAPI for illustrations
- Tailwind v4 â†’ v3 downgrade required (v4 uses different config format)
- PostCSS configs needed ESM format for Vite 7
- Phase 3 demo video: real app screenshots + agent narration (6.1MB)
- App walkthrough video: silent 30s footage for narration overlay

#### 17:40â€“18:00 â€” FLUX Fine-Tuning Pipeline
- **Doc researched and delivered FINETUNE_PLAN.md** via team.py (Firefly consultation)
- Decision: LoRA approach (not DreamBooth/full fine-tune) â€” fits 24GB M4
- **Pathfinder** generated 20 style-consistent training prompts:
  - 5 animal characters (fox, bunny, owl, kitten, bear cub)
  - 5 magical landscapes (enchanted forest, starry sky, flower garden, snow village, underwater)
  - 5 character-in-scene composites
  - 5 close-ups (animal face, magical object, glowing flower, storybook, lantern)
  - All prompts include: "children book illustration, watercolor style, soft warm lighting, dreamy atmosphere, kid-friendly, bedtime story aesthetic"
- Training environment: Python 3.12 venv, torch 2.10, diffusers 0.36, peft 0.18, MPS backend
- Scripts written: `train_lora.py`, `log_to_wandb.py`, `compare_models.py`, `auto_train.sh`
- **Automated pipeline:** Image gen â†’ auto-detect completion â†’ stop Ollama â†’ train â†’ restart Ollama
- Training images generating in background (~60min for 20 images via mflux)
- Training will start automatically when images complete (~18:55 AEST)
- Estimated training time: 1-2 hours (500 steps, rank 16)

---

## Technical Architecture

```
User â†’ React Frontend (Vite + Tailwind)
         â†“ POST /api/story
     FastAPI Backend
         â†“ asyncio.create_subprocess_exec
     team.py â†’ Mistral Conversations API â†’ Pathfinder Agent
         â†“ FunctionCallEntry.arguments (structured story JSON)
     Backend parses + saves to SQLite (aiosqlite)
         â†“ POST /api/narrate
     ElevenLabs (eleven_multilingual_v2, Adam voice)
         â†“ MP3 audio bytes
     Frontend plays via <Audio> element

Illustrations: FLUX schnell (local, mflux on Apple Silicon M4)
                â†’ LoRA fine-tuned for storybook style
```

## Agent Communication Flow
```
Doc (orchestrator)
  â†’ team.py pathfinder "Generate story for Luna..."
    â†’ Mistral Conversations API
    â†’ Pathfinder agent processes
    â†’ FunctionCallEntry with create_bedtime_story(title, scenes)
    â†’ team.py parses .arguments, outputs JSON
  â†’ main.py reads stdout, saves to SQLite

Doc
  â†’ team.py lifeline "What voice settings for multilingual TTS?"
    â†’ Lifeline agent researches
    â†’ MessageOutputEntry with recommendations
    â†’ team.py outputs text

Doc
  â†’ team.py firefly "What's the architecture for this app?"
    â†’ Firefly agent designs
    â†’ MessageOutputEntry with 26-file plan
```

## Key Numbers
- **4** Mistral Agents (Doc, Pathfinder, Firefly, Lifeline)
- **3** demo stories (EN, JA, FR) with 6 scenes each
- **3** FLUX illustrations (scene 1 per story)
- **20** training images for LoRA fine-tuning
- **6** API endpoints
- **3** frontend screens (Creator, Player, Library)
- **6** bugs found and fixed during E2E testing
- **12** git commits in ~4 hours
- **5** ElevenLabs voice segments per demo video
- **0** lines of code written by Claude/GPT â€” ALL via Mistral (Vibe CLI + Agents API)

## Mistral API Usage
- **Vibe CLI (devstral-2):** Frontend scaffold, backend wiring, bug fixes
- **Agents API (mistral-large-latest):** Story generation, voice research, architecture planning
- **Conversations API:** Persistent context per agent via conversation IDs stored in `.conversations.json`
- **FunctionCallEntry pattern:** Pathfinder's `create_bedtime_story` function returns structured data â€” more reliable than asking for JSON in text

## Prize Strategy
| Prize | Evidence |
|-------|----------|
| Sydney 1st Place | Full working app, multi-agent architecture, 3 languages |
| Best ElevenLabs Usage | Multilingual narration (EN/JA/FR/HI), eleven_multilingual_v2 |
| Best Agent Skills | 4-agent team with delegation via Conversations API |
| Best Vibe Usage | ALL code generated by Vibe CLI (devstral-2) |
| Best Architectural Modification | FLUX LoRA fine-tuning, FunctionCallEntry handling |
| Next Unicorns | Bedtime stories are a $2B market |

## Quotable Moments
- "An orchestrator without frictionless delegation tools is just a fancy single agent." â€” learned when Doc refused to use his team
- "We built team.py to make delegation easier than doing it yourself." â€” the fix
- "The bug wasn't the Mistral SDK, it was aiosqlite creating threads inside our async event loop." â€” 20 minutes of misdirected debugging
- "Pathfinder doesn't return text â€” it calls a function. The story data is in the function arguments, not the content." â€” Mistral Agents with tools behave differently
- "Adam speaks Japanese perfectly. You don't need language-specific voices with eleven_multilingual_v2." â€” ElevenLabs discovery

---

_This log is maintained by Loki (OpenClaw agent) for Nissan Dookeran. Updated in real-time during the hackathon._
_Last updated: 2026-02-28 18:05 AEST_

#### 18:00â€“18:30 â€” Gemini Training Data + Tailscale Funnel

**Training Data Pivot: FLUX â†’ Gemini Nano Banana Pro**
- Original plan: generate training images with FLUX itself (~3min/image, 60min total)
- Problem: circular â€” training a model on its own output doesn't improve quality
- **Pivot:** Use Gemini 3 Pro Image (Nano Banana Pro) for training data
- Result: 20 images in ~3 minutes, 1K resolution, ~2MB each, professional storybook watercolor quality
- **Blog angle:** "Two frontier models collaborating: Gemini provides the quality ceiling, FLUX becomes the private local inference engine"

**LoRA Training Challenges**
- FLUX.1-schnell full pipeline = ~18GB â€” doesn't fit alongside other services
- Stopped Docker to free ~5GB RAM
- First attempt: 401 on HF download â€” training venv didn't have HF_TOKEN â†’ fixed with `huggingface_hub.login()`
- Second attempt: OOM â€” 30.12GB allocated on MPS (entire unified memory of 24GB Mac Mini)
- **Mitigation:** Lower rank (8â†’4), lower resolution (512â†’256), disable MPS watermark

**Tailscale Funnel**
- Nissan enabled via admin console link
- Backend API: `https://nissans-mac-mini.tailc49510.ts.net/api/stories`
- Frontend app: `https://nissans-mac-mini.tailc49510.ts.net/app`
- Frontend API URLs made relative (works via Funnel or localhost)
- Built frontend production bundle served by FastAPI at `/app/*`

**Key lesson:** Fine-tuning FLUX on Apple Silicon 24GB is at the edge of feasibility. The 12B parameter model + LoRA + VAE + text encoders need careful memory management. Real production would need 48GB+ or cloud GPU.
