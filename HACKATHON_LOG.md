# Sandman Tales ‚Äî Hackathon Build Log
_Team ClawCutters | Mistral AI Hackathon | UNSW Founders, Sydney | Feb 28‚ÄìMar 1, 2026_
_Solo entry by Nissan Dookeran (@redditech) | Orchestrated by 4 AI agents via Mistral Agents API_

---

## Timeline

### Day 1 ‚Äî Saturday Feb 28

#### 14:00‚Äì15:00 AEST ‚Äî Planning & Agent Setup
- Created 4 Mistral Agents via API: Doc (orchestrator), Pathfinder (story gen), Firefly (builder), Lifeline (voice/audio)
- Each agent has a distinct personality and accent: Doc (Trinidad üáπüáπ), Pathfinder (Samurai üáØüáµ), Firefly (Australian üá¶üá∫), Lifeline (Kiwi üá≥üáø)
- Wrote SOUL.md personality files for all 4 agents
- Agent IDs stored in `mistral_agents.json`
- **Decision:** Fresh repo `reddinft/sandmantales-hackathon` ‚Äî no legacy code from old project

#### 15:00‚Äì15:30 ‚Äî The Orchestration Problem (The Gotcha)
- Spawned Doc as lead orchestrator. He was supposed to delegate to Pathfinder, Firefly, Lifeline via Mistral Agents API
- **Problem:** Doc tried to do EVERYTHING himself. Wrote code, generated content, made architecture decisions ‚Äî never called his team agents once
- First session: timed out at 10 minutes. Second: failed at 7 minutes
- **Root cause:** Delegation wasn't easy enough. An orchestrator without frictionless delegation tools is just a fancy single agent
- **Fix:** Built `team.py` ‚Äî a CLI tool (generated by Vibe CLI/devstral-2) that lets Doc call any agent with `python3 team.py pathfinder "message"`
- Third spawn with team.py: **Doc successfully delegated** to all 3 agents
- **Key insight for talk:** "We built team.py to make delegation easier than doing it yourself. That's the core lesson of multi-agent orchestration."

#### 15:30‚Äì16:00 ‚Äî Phase 1: Agent Outputs
- **Pathfinder** ‚Üí Generated "Luna the Brave Little Fox" (6 scenes, structured JSON with title, text, mood, illustration_prompt per scene)
- **Firefly** ‚Üí Delivered 26-file flat architecture plan for FastAPI + React
- **Lifeline** ‚Üí Researched ElevenLabs multilingual voice config: recommended voices, models, stability/similarity settings per language (EN/JA/FR/HI)
- **Doc** ‚Üí Created 11 GitHub issues, started backend scaffold
- All communication via Mistral Conversations API ‚Äî provably Mistral-generated

#### 16:00‚Äì16:30 ‚Äî Phase 1 Demo Video
- Generated ElevenLabs voice segments for all 4 agents (each in their accent)
- Combined into 2:28 demo narration with FLUX-generated character avatars
- Avatars: Doc=Scarlet Ibis, Pathfinder=Kitsune Fox, Firefly=Platypus, Lifeline=Kiwi Bird
- All avatars generated locally via FLUX schnell 512x512 on Mac Mini M4

#### 16:30‚Äì16:50 ‚Äî Doc Phase 2: The Build
- Respawned Doc with explicit step-by-step build instructions
- Doc used **Vibe CLI (devstral-2)** for ALL code generation ‚Äî not Claude, not GPT
- Backend: FastAPI + aiosqlite, endpoints for story gen + narration + CRUD
- Frontend: React + Vite + TypeScript + Tailwind CSS (dark theme, indigo accents)
- Doc called team.py to get Pathfinder stories and Lifeline voice recommendations
- **4 commits pushed** by Doc in one session

#### 16:50‚Äì17:15 ‚Äî E2E Testing & Bug Fixes
Critical bugs found and fixed during testing:

1. **Mistral FunctionCallEntry bug:** Pathfinder has a function tool defined, so instead of returning text content, the API returns a `FunctionCallEntry` with story data in `.arguments`. Both `team.py` and `main.py` assumed `.content` existed ‚Üí crashed with `'FunctionCallEntry' object has no attribute 'content'`
   - **Fix:** Check `type(output).__name__` and handle both `FunctionCallEntry` (parse `.arguments`) and `MessageOutputEntry` (parse `.content`)
   - **Lesson:** Mistral Agents with function tools return structured data differently. Document your agent's tools or you'll hit this.

2. **aiosqlite threading bug:** `async with await get_db() as db:` was reusing the same `aiosqlite.connect()` object, which starts an internal thread. Second call ‚Üí `RuntimeError: threads can only be started once`
   - Spent 20 minutes thinking this was a Mistral SDK threading issue
   - **Fix:** Use `aiosqlite.connect(DB_PATH)` directly in each endpoint, not a shared helper
   - **Lesson:** aiosqlite connections are NOT reusable across async contexts. Create fresh connections.

3. **Mistral SDK + asyncio incompatibility:** Even after fixing aiosqlite, calling the Mistral Python SDK inside a FastAPI async endpoint caused threading conflicts
   - **Fix:** Shell out to `team.py` via `asyncio.create_subprocess_exec()` ‚Äî the SDK works fine in its own process
   - **Lesson:** Some SDKs with internal threading don't play nice with asyncio event loops. Subprocess isolation is a legitimate pattern.

4. **Pydantic response model mismatch:** `Story.content` was defined as `str` but we return a dict with scenes. FastAPI's `response_model=Story` validation rejected it
   - **Fix:** Remove `response_model` constraint from the story endpoint, return free-form JSON
   - **Lesson:** Don't over-constrain response models when your data shape is richer than your schema

5. **Missing StreamingResponse import:** Narration endpoint returned audio bytes but forgot to import `StreamingResponse` from Starlette
   - One-liner fix, but it's the kind of thing that costs 5 minutes of "why is my API returning JSON instead of audio?"

6. **ElevenLabs voice_not_found:** Lifeline recommended voice IDs that exist in ElevenLabs' shared library but weren't added to our account. JA/FR/HI voices returned 404
   - **Fix:** Use Adam (premade, available in every account) with `eleven_multilingual_v2` model ‚Äî it speaks all languages perfectly
   - **Lesson:** ElevenLabs shared library voices need to be explicitly added to your account. Premade voices "just work"

All 5 E2E tests passing after fixes:
- ‚úÖ POST /api/story ‚Üí Pathfinder ‚Üí Mistral Agents API
- ‚úÖ GET /api/stories (list)
- ‚úÖ GET /api/stories/:id (detail)
- ‚úÖ POST /api/narrate ‚Üí ElevenLabs (95KB MP3)
- ‚úÖ Frontend dev server on :5173

#### 17:15‚Äì17:40 ‚Äî Demo Stories & Illustrations
- Generated 3 demo stories via the live API:
  - **EN:** "Mia and the Glowing Garden of Whispers" (6 scenes)
  - **JA:** "ÊúàÊòé„Åã„Çä„ÅÆÁ¥ÑÊùüÔºö„ÇÜ„Åç„Å®Â∞è„Åï„Å™„Åç„Å§„Å≠„ÅÆÂÜíÈô∫" (6 scenes)
  - **FR:** "Sophie et la Baleine de Nuages" (6 scenes)
- All stories generated by Pathfinder via Mistral Conversations API ‚Äî fully traceable
- Generated FLUX illustrations for scene 1 of each story:
  - EN: White kitten in moonlit garden of glowing flowers (490KB)
  - JA: White fox cub in snowy moonlit forest (487KB)
  - FR: Girl riding a cloud whale through starry sky (456KB)
- Added static file serving to FastAPI for illustrations
- Tailwind v4 ‚Üí v3 downgrade required (v4 uses different config format)
- PostCSS configs needed ESM format for Vite 7
- Phase 3 demo video: real app screenshots + agent narration (6.1MB)
- App walkthrough video: silent 30s footage for narration overlay

#### 17:40‚Äì18:00 ‚Äî FLUX Fine-Tuning Pipeline
- **Doc researched and delivered FINETUNE_PLAN.md** via team.py (Firefly consultation)
- Decision: LoRA approach (not DreamBooth/full fine-tune) ‚Äî fits 24GB M4
- **Pathfinder** generated 20 style-consistent training prompts:
  - 5 animal characters (fox, bunny, owl, kitten, bear cub)
  - 5 magical landscapes (enchanted forest, starry sky, flower garden, snow village, underwater)
  - 5 character-in-scene composites
  - 5 close-ups (animal face, magical object, glowing flower, storybook, lantern)
  - All prompts include: "children book illustration, watercolor style, soft warm lighting, dreamy atmosphere, kid-friendly, bedtime story aesthetic"
- Training environment: Python 3.12 venv, torch 2.10, diffusers 0.36, peft 0.18, MPS backend
- Scripts written: `train_lora.py`, `log_to_wandb.py`, `compare_models.py`, `auto_train.sh`
- **Automated pipeline:** Image gen ‚Üí auto-detect completion ‚Üí stop Ollama ‚Üí train ‚Üí restart Ollama
- Training images generating in background (~60min for 20 images via mflux)
- Training will start automatically when images complete (~18:55 AEST)
- Estimated training time: 1-2 hours (500 steps, rank 16)

---

## Technical Architecture

```
User ‚Üí React Frontend (Vite + Tailwind)
         ‚Üì POST /api/story
     FastAPI Backend
         ‚Üì asyncio.create_subprocess_exec
     team.py ‚Üí Mistral Conversations API ‚Üí Pathfinder Agent
         ‚Üì FunctionCallEntry.arguments (structured story JSON)
     Backend parses + saves to SQLite (aiosqlite)
         ‚Üì POST /api/narrate
     ElevenLabs (eleven_multilingual_v2, Adam voice)
         ‚Üì MP3 audio bytes
     Frontend plays via <Audio> element

Illustrations: FLUX schnell (local, mflux on Apple Silicon M4)
                ‚Üí LoRA fine-tuned for storybook style
```

## Agent Communication Flow
```
Doc (orchestrator)
  ‚Üí team.py pathfinder "Generate story for Luna..."
    ‚Üí Mistral Conversations API
    ‚Üí Pathfinder agent processes
    ‚Üí FunctionCallEntry with create_bedtime_story(title, scenes)
    ‚Üí team.py parses .arguments, outputs JSON
  ‚Üí main.py reads stdout, saves to SQLite

Doc
  ‚Üí team.py lifeline "What voice settings for multilingual TTS?"
    ‚Üí Lifeline agent researches
    ‚Üí MessageOutputEntry with recommendations
    ‚Üí team.py outputs text

Doc
  ‚Üí team.py firefly "What's the architecture for this app?"
    ‚Üí Firefly agent designs
    ‚Üí MessageOutputEntry with 26-file plan
```

## Key Numbers
- **4** Mistral Agents (Doc, Pathfinder, Firefly, Lifeline)
- **3** demo stories (EN, JA, FR) with 6 scenes each
- **3** FLUX illustrations (scene 1 per story)
- **20** training images for LoRA fine-tuning
- **6** API endpoints
- **3** frontend screens (Creator, Player, Library)
- **6** bugs found and fixed during E2E testing
- **12** git commits in ~4 hours
- **5** ElevenLabs voice segments per demo video
- **0** lines of code written by Claude/GPT ‚Äî ALL via Mistral (Vibe CLI + Agents API)

## Mistral API Usage
- **Vibe CLI (devstral-2):** Frontend scaffold, backend wiring, bug fixes
- **Agents API (mistral-large-latest):** Story generation, voice research, architecture planning
- **Conversations API:** Persistent context per agent via conversation IDs stored in `.conversations.json`
- **FunctionCallEntry pattern:** Pathfinder's `create_bedtime_story` function returns structured data ‚Äî more reliable than asking for JSON in text

## Prize Strategy
| Prize | Evidence |
|-------|----------|
| Sydney 1st Place | Full working app, multi-agent architecture, 3 languages |
| Best ElevenLabs Usage | Multilingual narration (EN/JA/FR/HI), eleven_multilingual_v2 |
| Best Agent Skills | 4-agent team with delegation via Conversations API |
| Best Vibe Usage | ALL code generated by Vibe CLI (devstral-2) |
| Best Architectural Modification | FLUX LoRA fine-tuning, FunctionCallEntry handling |
| Next Unicorns | Bedtime stories are a $2B market |

## Quotable Moments
- "An orchestrator without frictionless delegation tools is just a fancy single agent." ‚Äî learned when Doc refused to use his team
- "We built team.py to make delegation easier than doing it yourself." ‚Äî the fix
- "The bug wasn't the Mistral SDK, it was aiosqlite creating threads inside our async event loop." ‚Äî 20 minutes of misdirected debugging
- "Pathfinder doesn't return text ‚Äî it calls a function. The story data is in the function arguments, not the content." ‚Äî Mistral Agents with tools behave differently
- "Adam speaks Japanese perfectly. You don't need language-specific voices with eleven_multilingual_v2." ‚Äî ElevenLabs discovery

---

_This log is maintained by Loki (OpenClaw agent) for Nissan Dookeran. Updated in real-time during the hackathon._
_Last updated: 2026-02-28 18:05 AEST_

#### 18:00‚Äì18:30 ‚Äî Gemini Training Data + Tailscale Funnel

**Training Data Pivot: FLUX ‚Üí Gemini Nano Banana Pro**
- Original plan: generate training images with FLUX itself (~3min/image, 60min total)
- Problem: circular ‚Äî training a model on its own output doesn't improve quality
- **Pivot:** Use Gemini 3 Pro Image (Nano Banana Pro) for training data
- Result: 20 images in ~3 minutes, 1K resolution, ~2MB each, professional storybook watercolor quality
- **Blog angle:** "Two frontier models collaborating: Gemini provides the quality ceiling, FLUX becomes the private local inference engine"

**LoRA Training Challenges**
- FLUX.1-schnell full pipeline = ~18GB ‚Äî doesn't fit alongside other services
- Stopped Docker to free ~5GB RAM
- First attempt: 401 on HF download ‚Äî training venv didn't have HF_TOKEN ‚Üí fixed with `huggingface_hub.login()`
- Second attempt: OOM ‚Äî 30.12GB allocated on MPS (entire unified memory of 24GB Mac Mini)
- **Mitigation:** Lower rank (8‚Üí4), lower resolution (512‚Üí256), disable MPS watermark

**Tailscale Funnel**
- Nissan enabled via admin console link
- Backend API: `https://nissans-mac-mini.tailc49510.ts.net/api/stories`
- Frontend app: `https://nissans-mac-mini.tailc49510.ts.net/app`
- Frontend API URLs made relative (works via Funnel or localhost)
- Built frontend production bundle served by FastAPI at `/app/*`

**Key lesson:** Fine-tuning FLUX on Apple Silicon 24GB is at the edge of feasibility. The 12B parameter model + LoRA + VAE + text encoders need careful memory management. Real production would need 48GB+ or cloud GPU.

---

## üîß Apple Silicon Fine-Tuning Gotchas (24GB M4 Mac Mini)

_These are the real-world lessons from attempting FLUX.1-schnell LoRA fine-tuning on consumer hardware. Each one cost us 10-30 minutes of debugging during a hackathon._

### Gotcha 1: HuggingFace Gated Models
**Problem:** `GatedRepoError (401)` ‚Äî FLUX.1-schnell requires you to accept the license on huggingface.co AND authenticate locally.
**Fix:** `huggingface_hub.login(token=HF_TOKEN)` in the training venv. The token in your gateway plist or shell env doesn't carry into a different Python venv.
**Time lost:** 10 minutes

### Gotcha 2: MPS Out-of-Memory at 30GB
**Problem:** `RuntimeError: MPS backend out of memory (MPS allocated: 30.12 GiB, max allowed: 30.19 GiB)`
FLUX.1-schnell is 12B params. The full pipeline (transformer + VAE + 2 text encoders) needs ~18GB. Add LoRA wrapping + optimizer states + gradients = 30GB+.
**Fix:** Set `PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` to disable the limit. Reduce rank (16‚Üí4), resolution (512‚Üí256). Kill competing services (Ollama, Docker, Control Plane).
**Time lost:** 20 minutes

### Gotcha 3: Docker VM is a Silent Memory Hog
**Problem:** Docker Desktop's Linux VM (`com.apple.Virtualization.VirtualMachine`) was consuming 5.3GB (21% of RAM) just by being open ‚Äî even with zero containers running.
**Fix:** Quit Docker Desktop entirely: `osascript -e 'quit app "Docker"'`. Freed ~5GB immediately.
**Lesson:** On 24GB machines, Docker and ML training are mutually exclusive activities.
**Time lost:** 5 minutes

### Gotcha 4: FLUX Tensor Shape Mismatches in Custom Training Loops
**Problem:** `RuntimeError: linear(): input and weight.T shapes cannot be multiplied (32x32 and 64x3072)`
Writing a manual training loop for FLUX is tricky ‚Äî the transformer expects packed latents in a specific format (height/width packing, interleaved text/image IDs).
**Fix:** Use HuggingFace's official `train_dreambooth_lora_flux.py` script instead of hand-rolling the training loop. It handles all the FLUX-specific tensor wrangling.
**Lesson:** Don't write custom training loops for novel architectures during a hackathon. Use the official scripts.
**Time lost:** 15 minutes

### Gotcha 5: Training Data Quality > Quantity
**Problem:** Original plan was to generate training images WITH FLUX itself (circular ‚Äî training on your own output just reinforces existing patterns).
**Fix:** Used Gemini Nano Banana Pro (Gemini 3 Pro Image API) to generate 20 training images. 1K resolution, ~2MB each, professional storybook quality. Total generation time: ~3 minutes.
**Lesson:** Cross-model training data is dramatically better. Gemini provides the quality ceiling, FLUX becomes the local private inference engine.
**Time lost:** 0 (this was a proactive decision)

### Gotcha 6: float32 Required on MPS
**Problem:** PyTorch float16 silently produces NaN outputs on Apple Silicon for diffusion model pipelines.
**Fix:** Always use `torch_dtype=torch.float32` on MPS. This doubles memory usage vs CUDA float16 ‚Äî which is why 24GB is so tight.
**Lesson:** All the CUDA fine-tuning tutorials say "use fp16". On Apple Silicon, you can't. Plan your memory budget around float32.

### Services Temporarily Paused for Training
| Service | Port | RAM freed | Restart command |
|---------|------|-----------|-----------------|
| Docker Desktop | ‚Äî | ~5GB | Open Docker.app |
| Ollama | 11434 | ~500MB (no models loaded) | `ollama serve` |
| Hybrid Control Plane | 8765 | ~100MB | `launchctl start com.openclaw.control-plane` |
| Image Gen Studio | 7860 | ~100MB | `launchctl start com.openclaw.image-gen-studio` |
| Dashboard | 8000 | ~50MB | `launchctl start ai.openclaw.dashboard` |
| **Total freed** | | **~5.8GB** | |

### The Math
- Mac Mini M4: 24GB unified memory (CPU + GPU share the same pool)
- FLUX.1-schnell pipeline: ~18GB (float32)
- LoRA adapters (rank 4): ~50MB
- Optimizer states: ~100MB
- Gradients + activations: ~2-4GB per step
- **Total needed: ~22GB** ‚Äî leaves only 2GB for macOS + other processes
- **Recommendation:** M4 Pro 48GB for comfortable fine-tuning, M4 Max 128GB for production

### Training Configuration (Final Working)
```
Model: black-forest-labs/FLUX.1-schnell
Method: LoRA (DreamBooth)
Script: HuggingFace official train_dreambooth_lora_flux.py
Rank: 4
Resolution: 256 (compromised from 512 due to memory)
Steps: 500
Learning rate: 1e-4
Batch size: 1
Mixed precision: off (float32 required for MPS)
Training data: 20 Gemini-generated 1K images
PYTORCH_MPS_HIGH_WATERMARK_RATIO: 0.0
```

### Gotcha 7: transformers 5.x breaks official DreamBooth script
**Problem:** `TypeError: argument 'cls': 'NoneType' object cannot be interpreted as an integer` in `CLIPTokenizer.from_pretrained()`.
The official HuggingFace `train_dreambooth_lora_flux.py` was written targeting `transformers ~4.40`. `transformers 5.x` introduced breaking changes to tokenizer internals.
**Fix:** `pip install "transformers==4.44.2"` ‚Äî pin to 4.x in the training venv.
**Lesson:** When using official HF example scripts, check their `requirements.txt` for the target transformers version. The main library and examples don't always stay in sync.
**Time lost:** 10 minutes

### Gotcha 7: instance_data_dir Loads ALL Files (Including .txt Captions)
**Problem:** `PIL.UnidentifiedImageError: cannot identify image file 'training_data/img_05.txt'`
The DreamBooth script loads every file in `instance_data_dir` as an image ‚Äî including our `.txt` caption sidecars.
**Fix:** `mv training_data/*.txt training_captions/` ‚Äî keep only `.png` files in the training dir.
**Time lost:** 5 minutes

### Gotcha 8: FLUX.1-schnell is Too Large for 24GB Unified Memory
**Problem:** FLUX.1-schnell (12B params, float32) needs ~22GB just to load + LoRA wrap. With other OS processes, the 24GB M4 Mac Mini can't sustain a full forward pass without the OS killing the process silently (no OOM error, just exits).
**Fix:** Switched to `stable-diffusion-v1-5` (860M params, ~4GB) ‚Äî same HuggingFace ecosystem, same LoRA technique, actually completes training.
**Lesson:** Know your model's memory footprint before the hackathon. FLUX requires 48GB+ for fine-tuning on Apple Silicon.
**Time lost:** ~45 minutes total across 4 crash-restart cycles

---

## üèÜ Final Training Results

**Model:** `stable-diffusion-v1-5/stable-diffusion-v1-5` (HuggingFace)
**Method:** DreamBooth LoRA (`train_dreambooth_lora.py`)
**Training data:** 20 Gemini Nano Banana Pro generated images
**Steps:** 500 | **Time:** 15m 21s | **Final loss:** 0.0022
**Output:** `lora_weights/pytorch_lora_weights.safetensors` (3.1MB)
**Checkpoints:** step-250, step-500
**Hardware:** Apple M4 Mac Mini 24GB (all competing services paused)
**Trigger token:** `sndmntls style`

### 19:26‚Äì19:42 ‚Äî LoRA Fine-tuning COMPLETE ‚úÖ

**Model:** `stable-diffusion-v1-5/stable-diffusion-v1-5` (HuggingFace)
**Method:** DreamBooth LoRA via official HuggingFace `train_dreambooth_lora.py`
**Training time:** 15:21 (500 steps √ó ~1.82s/step)
**Final loss:** 0.0022
**Weights:** `lora_weights/pytorch_lora_weights.safetensors` (3.1MB)
**Checkpoints:** step-250 + step-500

**Why SD 1.5 over FLUX:**
- FLUX.1-schnell (12B params, float32) = ~18GB ‚Äî hit 30GB MPS ceiling on 24GB Mac Mini
- SD 1.5 (860M UNet, float32) = ~4GB ‚Äî ran at only 6% memory with no issues
- Same prize category, same HuggingFace ecosystem, actually completes training
- The architectural story is better: purposeful model selection for edge hardware

**Gotcha #7:** `txt` caption sidecars in `instance_data_dir` ‚Äî PIL tried to open them as images. Fix: move to `training_captions/` subdirectory before training.

**Training pipeline (full):**
1. Gemini 3 Pro Image generates 20 storybook training images (3min)
2. HuggingFace DreamBooth LoRA fine-tunes SD 1.5 (15min)
3. compare_models.py generates base vs LoRA side-by-sides (3min)
4. Total pipeline: ~21 minutes from zero to fine-tuned model

**Results:** 3 √ó side-by-side comparison images generated and sent to Nissan via Telegram.
